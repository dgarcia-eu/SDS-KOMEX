<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Computational Affective Science and Sentiment Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="David Garcia    University of Konstanz" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="libs/footer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Computational Affective Science and Sentiment Analysis
]
.author[
### David Garcia <br><br> <em>University of Konstanz</em>
]
.date[
### Social Data Science with Python - KOMEX
]

---







layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;David Garcia - Social Data Science with Python&lt;/span&gt;&lt;/div&gt; 

---

# Outline

## 1. Basics of dictionary methods

## 2. Measuring emotions

## 3. Dictionary methods in sentiment analysis

## 4. Supervised sentiment analysis

---

# Dictionary methods: the General Inquirer

&lt;div style="float:right"&gt;
&lt;img src="GI.jpg" alt="General Inquirer" width="320px"/&gt;  
&lt;/div&gt;

The pioneer work of [Philip Stone in 1966](https://mitpress.mit.edu/books/general-inquirer
) proposed to process text with a computer to detect the use of words of various categories. This set the basis for **dictionary methods** in text analysis, which are based on counting the number of appearances of the words of a list in a text. 

The lists of [positive words](http://www.wjh.harvard.edu/~inquirer/Positiv.html) and of [negative words](http://www.wjh.harvard.edu/~inquirer/Negativ.html) of this version, which served as input for later methods like [SentiStrength](http://sentistrength.wlv.ac.uk/) (more on this later).

The [SentimentAnalysis R package](https://cran.r-project.org/web/packages/SentimentAnalysis/index.html) contains the General Inquirer (GI) dictionary and methods to match words in text.

---

# The assumption: bag of words

.pull-left[1. Tokenize text to identify words and expressions, for example identifing whitespace and punctuation
2. Count the number of tokens of each kind in each text (term frequency)

- Result: each document is represented by a vector of word counts
- Ignores word order or relationship between words]

.pull-right[.center[![:scale 70%](BOW.jpg)]]

---

# Bag of words example

| Text      | he | sat | on | the | bank | bear | is | brown | did  | not|  survive |  crisis | didn't |
| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |
| He sat on the bank      | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0  |  0 |  0 | 0 |0 | 
| The bear is brown   | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 0  | 0|  0 | 0 | 0 | 
| The bank did not survive the crisis | 0 | 0 | 0 | 2 | 1 | 0 | 0 | 0 | 1  | 1|  1 | 1 | 0 | 
| He didn't sat on the bear | 1 | 1 | 1 | 1| 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |
---

## Linguistic Inquiry and Word Count (LIWC)

.center[![:scale 82%](LIWCexample.png)]  
LIWC (pronounced "Luke") was developed as a click-and-run software by [James Pennebaker in 2001](https://liwc.wpengine.com/), including word lists for emotions and ther classes.

---

# Examples of word classes in LIWC

- `\(funct_1\)`: **Function words**, words that do not carry strong meaning (structure)
  - `\(i_4\)`: **First-person references**, especially pronouns
  

- `\(affect_{125}\)`: **Affective words**, words signalling emotional experiences
  - `\(negemo_{127}\)`: **Negative emotion words**
    - `\(anx_{128}\)`: **Anxiety words**, words signalling fear, stress and anxiety


- `\(social_{121}\)`: **Social process words**, words about others, communities, and social activities


- `\(cogmech_{131}\)`: **Cognitive mechanisms**
  - `\(insight_{132}\)`: thinking and information processing
  - `\(incl_{138}\)`: inclusion terms, synthesis
  - `\(discrep_{134}\)`: discrepancy, identification of opposites

---

## Measuring emotions

## 1. Basics of dictionary methods

## *2. Measuring emotions*

## 3. Dictionary methods in sentiment analysis

## 4.  Supervised sentiment analysis

---

# What are emotions?

&gt; Emotions as **core affect**: Short-lived psychological states that consume the individual's energy and strongly influence cognition and behavior, for example expression.

Emotional or affective behavior of an individual takes place at various timescales:

![](emotionscale.png)
- Reflex reactions: fast physiological responses  
- Core affect: relax quickly and are triggered by a stimulus
- Mood: slow-changing and constant emotional state 
- Personality traits are lifelong behavior patterns, some about emotions

---

# Computational Affective Science

&gt; **Affective Science** is the (interdisciplinary) scientific study of emotions. 

&gt; **Computational Affective Science** applies methods from Computer Science and Data Science to Affective Science. Some examples are:

- **Affective Computing:** Development of systems that detect, process, and elicit emotion  
- **Cyberpsychology of Affect:** Understanding the interplay between emotions and ICT  
- **Emotion Recognition:** Identification of human emotion using any kind of modality: text, voice, facial expression, physiological signals (skin conductance, muscle activity, EEG, fMRI), etc  
- **Sentiment Analysis:** Detection of subjective states from (textual) data, including emotion

---

# Measuring emotions

Emotions can be measured through various signals and observable behaviors:

![](measurement.png)

In the following, we are going to cover four models of how to capture emotions in quantitative research. Some approaches are better for some modes or signals (e.g. text, facial expression) than others.

---

# Ekman's basic emotions model

.center[![:scale 48%](ekman.png)]
Developed by **Paul Ekman** to classify facial expression of emotions. 
---

# Plutchik's wheel of emotions
.center[![:scale 47%](Wheel.png)]

---

# The circumplex model of affect

.center[![:scale 47%](circumplex.png)]

---

# Dimensions in the circumplex model


&gt; **Valence:** the degree of pleasure experienced in an emotion  
  
- Explains the most variance from positive/pleasant to negative/unpleasant
- It can be measured physiologically with smiling and frowning muscle activity
- It is the most common dimension of emotions included in text analysis  

&gt; **Arousal:** the level of activity associated with an emotion  

- Explains less variance than valence but it is informative to differentiate emotions  
- It can be measured with skin conductance and heart rate sensors  
- Not so common in text analysis but can be estimated from voice tone  

---

# Positive And Negative Affect Schedule

.center[![:scale 70%](PANAS2.jpg)]

---


## Dictionary methods in sentiment analysis

## 1. Basics of dictionary methods

## 2. Measuring emotions

## *3. Dictionary methods in sentiment analysis*

## 4.  Supervised sentiment analysis


---

#  What is Sentiment Analysis?

&gt; **Sentiment Analysis:** Computerized quantification of subjective states from text

.center[![](SentimentSchema.png)]

- Examples of subjective states: Emotions, feelings, attitudes, opinions...  
- Often vaguely defined and roughly equivalent to the dimension of valence in the circumplex model
- Sentiment quantification can have various formats: polarity, scores, labels...  

---

# The Sentiment Analysis Boom

&lt;img src="Slides_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---

# Supervised vs Unsupervised Methods

- **Unsupervised sentiment analysis:**  
  - Uses expert knowledge (e.g. from psychologists) to quantify emotions  
  - Expert knowledge is encoded as a set of rules or a lexicon (dictionary) of words. Also known as "dictionary methods"  
  - Pros: Simple implementation, large coverage and recall  
  - Cons: Hard to customize for a particular context, low precision, expert bias

- **Supervised sentiment analysis (next week):**  
  - Uses a set of annotated texts to fit a model  
  - Annotations can come from readers or the authors of texts  
  - Pros: Automatic calibration, high precision  
  - Cons: Lower recall and coverage, need very large training datasets  

---

## Counting positive and negative words

.center[![:scale 100%](MPQAex.png)]

- Methods similar to LIWC that count the number of positive and negative words
- Example: [Multi-Perspective Question Answering (MPQA) subjectivity lexicon](https://mpqa.cs.pitt.edu/lexicons/)
- [Bing Liu opinion lexicon for product reviews](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)

---

## Averaging valence ratings: The hedonometer

.center[![:scale 78%](ANEWclassif.svg)]

[Measuring the happiness of large-scale written expression: Songs, blogs, and presidents. P. Dodds &amp; C. Danforth (2010)](https://psycnet.apa.org/record/2010-14167-004)

---
 # Counting emotion words: NRC lexicon

.center[![:scale 100%](NRC.png)]

Lexicon with words associated to Plutchik's wheel emotions plus positive/negative. Various additional versions including valence and translations.

[Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.](https://arxiv.org/pdf/1308.6297.pdf)

---

# Applying modifiers: SentiStrength

.center[![](SentiStrength.png)]
[Sentiment strength detection in short informal text. Thelwall, M., Buckley, K., Paltoglou, G. Cai, D., &amp; Kappas, A.  Journal of the American Society for Information Science and Technology (2010)](https://onlinelibrary.wiley.com/doi/10.1002/asi.21416)

---

# VADER 
&lt;div style="float:right"&gt;
&lt;img src="https://t-redactyl.io/figure/Vader_1.jpg" alt="VADER" width="395px"/&gt;  
&lt;/div&gt;

VADER (Valence Aware Dictionary and sEntiment Reasoner) is a tool very similar to SentiStrength in the steps it follows:

1. Text preprocessing  
2. Word matching from a lexicon of positive/negative scored words  
3. Application of modifiers to the scores based on language rules
 
VADER's name suggests it is the "dark version" of LIWC ("Luke"). As the authors of VADER say

[VADER: A parsimonious rule-based model for sentiment analysis of social media text. C Hutto, E Gilbert, ICWSM (2013)](https://ojs.aaai.org/index.php/icwsm/article/view/14550)

---

##  Example application: 9/11 Emotions in pagers 
.center[![:scale 95%](PsychSci.png)]

[The Emotional Timeline of September 11, 2001. Mitja D. Back, Albrecht C.P. Küfner, and Boris Egloff. Psychological Science (2010)](https://journals.sagepub.com/doi/full/10.1177/0956797610382124)

---
.center[![:scale 69%](anger911Timeline.png)]
---

## Not so angry americans
More than a third of anger words appeared in messages like these:
![:scale 100%](PagersExample.png)
"Reboot NT machine [name] in cabinet [name] at [location]:CRITICAL:[date and time]."  

The word "critical" is contained in the anger word list of LIWC!

---

### Anger timeline without REBOOT messages
.center[![:scale 87%](notSoAngry.png)]
----

## Supervised sentiment analysis

## 1. Basics of dictionary methods

## 2. Measuring emotions

## 3. Dictionary methods in sentiment analysis

## *4. Supervised sentiment analysis*

---


# Representations of text

**Text representation:** computing a fixed-length vector from a text to train a model on the features (values) of the vector

- **TF (Term Frequency) vector:** Using a bag of words assumption, represent each text as a vector of frequencies of the words in the language

- **TF-IDF (Term Frequency - Inverse Document Frequency):** Same but dividing by the total frequency of the term in all documents
  - Both TF and TF-IDF generate very long vectors (number of possible words) with many zeroes
  - Some approaches to remove rare words or uninformative stopwors
  - Requires design decisions: include punctuation? stem words?

- **Denser representations:** Word embeddings and transformer representations

---

# How to train your model
.center[![:scale 65%](supervisedPhases.png)] 

1. **Training:** Texts with annotations of sentiment are used to fit the model  
2. **Validation:** A second set of annotated texts not used for training are used to evaluate the quality of the model: `\(Q_{1}\)`  
3. **Testing:** One last evaluation run of the fitted model with a leave-out sample of annotated texts. None of these texts should have been part of the validation or training steps. Testing quality `\(Q_2\)` measures predictive quality of the model.
---

# Be extra careful with test sets
.center[![:scale 100%](TestTraining.png)] 

---

# Gold Standards and Ground Truths


pos | neg | text  
------------- | ------------- | -------------
0 | 0 |  I wana see the vid Kyan  
0 | 1 | I cant feel my feet.. that cant be good..  
1 | 0 | 10 absolutely jaw dropping concept car designs http://ow.ly/15OnX  
0 | 0 | Phil Collins- You Can’t Hurry Love

- Supervised sentiment analysis needs a set of labeled texts to learn from.
- Labels can come from the author of the text or from reading raters
- The above table is an example from a real dataset with two annotations: a positivity score and a negativity score
- Other ground truths  might have numeric scores in a scale or text labels for basic emotional states.

---

# Text preprocessing

.center[![:scale 75%](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535125878/NLTK3_zwbdgg.png)]

Pre-processing from [Text Analytics for Beginners using NLTK, Navlani, 2019](https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk)

---

# What model to use?

Common approaches are:  
1. **Naive Bayes:** Takes features as independent signals and fits the label according to Bayes Rule  
2. **Support Vector Machine:** Finds a separator given a (non-)linear combination of features  
3. **Random Forest:** Finds hierarchical decision rules that divide the texts in classes

In supervised sentiment analysis, generating the ground truth data is the most critical part and is required to train the model. Producing sufficient annotations from readers or authors can be expensive. Supervised methods are usually not out-of-the-box like unsupervised tools, you would have to fit your own model to a ground truth dataset.

---
&lt;div style="float:right"&gt;
&lt;img src="Precisionrecall.png" alt="Precision and Recall" width="480px"/&gt;
&lt;/div&gt;
# Evaluating classifiers 

- True positives `\(TP\)`: All positive cases that are correctly predicted

- False positives `\(FP\)`: All negatives that were wrongly predicted as positive

- True negatives `\(TN\)`: All negative cases that are correctly predicted 

- False negatives `\(FN\)`: All positive cases that were incorrectly predicted as negative

---

# Accuracy, Precision, and Recall
`$$Accuracy = \frac{TP+TN}{TP+FP+TN+FN}$$`
`$$Precision = \frac{TP}{TP+FP}$$`
`$$Recall = \frac{TP}{TP+FN}$$`  
- The measure of precision answers the question "How sure am I of this prediction?"
- The measure of recall answers the question  "How many of the things that I’m looking for are found?"

---

# Balancing precision and recall

A way to compute a trade-off between Precision and Recall is the `\(F_1\)` score, which is a harmonic mean of Precision and Recall:

`$$F_1= 2 ∗ \frac{ Precision ∗ Recall} {Precision + Recall}$$`

The `\(F_1\)` score is often used as a way to summarize the quality of classifiers. When more than one class is possible, you should look at the mean of `\(F_1\)` over the classess or to the `\(F_1\)` of each class separately. The `\(F_1\)` score is often used in sentiment analysis competitions to chose the best tools, for example in the [SemEval 2017 competition](https://alt.qcri.org/semeval2017/index.php?id=tasks).

---

# Let someone else do it: Black-box APIs

.center[![:scale 53%](IBMNLP.png)![:scale 47%](GoogleNLP.png)]

Easy to use but data and methods unknown. Do your own evaluation!

---

.center[![:scale 43%](Mind.jpeg)]

---

# Benchmarking sentiment analysis

.center[![:scale 100%](Benchmark.png)]
[SentiBench - a benchmark comparison of state-of-the-practice sentiment analysis methods
F. Ribeiro, et al. EPJ Data Science (2016)](https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-016-0085-1)

---

# Benchmark setup

.center[![:scale 80%](datasets.png)]
- 18 labeled datasets in three groups: Social networks, coomments, reviews
- 24 out-of-the box sentiment analysis methods
  - includes dictionary-based (LIWC, NRC)
  - rule-based (VADER, SentiStrength)
  - some supervised methods (SASA)
- Two tasks:
  - 2-class (positive/negative), given that it is not neutral
  - 3-class (positive/negative/neutral)
- Evaluation based on `\(F_1\)` score per class - summary as mean rank of methods



---
## Mean `\(F_1\)` across datasets
.center[![:scale 60%](SentiBench.jpg)]

---
# Example application: Trump2Cash

.pull-left[![:scale 90%](Trump2Cash.png)
- Google NLP API to classify sentiment about companies in Trump's tweets-
- Trading based on tweets: 59% annualized return (Feb 2017)

]  
.pull-right[![:scale 100%](Trump2CashPerformance.png)] 

---

# Summary

- **Basics of dictionary methods**
  - The idea: measuring word frequencies from dictionaries (word lists)
  - LIWC as one of the most popular methods


- **Measuring emotions**
  - Several models depending on modality and timescale
  - Models in text analysis: basic emotions, circumplex, PANAS


- **Dictionary methods in sentiment analysis**
  - Basic methods building on word counts
  - Methods using modifiers (amplifiers, negations): SentiStrength and VADER


- **Supervised sentiment analysis**
  - Training based on a document representation
  - Evaluation against annotations to choose models
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="libs/perc.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
